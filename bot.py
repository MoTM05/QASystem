import logging
import os
import asyncio
from typing import Optional

from telegram import Update, File as TelegramFile
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import \
    StreamingStdOutCallbackHandler
from langchain.text_splitter import RecursiveCharacterTextSplitter


BOT_TOKEN = "YOUR_TOKEN_HERE"
FAISS_INDEX_PATH = "faiss_AiDoc2"
MODEL_PATH = "models/llama-2-7b-chat.Q4_K_M.gguf"
EMBEDDING_MODEL_NAME = "hkunlp/instructor-large"
TEMP_UPLOAD_DIR = "temp_uploads_bot"


logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)


qa_system: Optional[RetrievalQA] = None
embeddings_model: Optional[HuggingFaceEmbeddings] = None


def get_embeddings():
    global embeddings_model
    if embeddings_model is None:
        logger.info(f"Initializing HuggingFaceEmbeddings with model: {EMBEDDING_MODEL_NAME}")
        embeddings_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    return embeddings_model


def initialize_llm():
    logger.info(f"Loading Llama-2 model from: {MODEL_PATH}")
    if not os.path.exists(MODEL_PATH):
        logger.error(f"Model file {MODEL_PATH} not found!")
        raise FileNotFoundError(f"Model file {MODEL_PATH} not found. Please download it.")

    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
    llm = LlamaCpp(
        model_path=MODEL_PATH,
        temperature=0.2,
        max_tokens=512,
        top_p=1,
        callback_manager=callback_manager,
        n_ctx=4096,
        verbose=False,
        n_gpu_layers=0
    )
    return llm


def check_faiss_index_exists(path: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ index.faiss –∏ index.pkl –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    index_file = os.path.join(path, "index.faiss")
    pkl_file = os.path.join(path, "index.pkl")
    return os.path.exists(index_file) and os.path.exists(pkl_file)


def initialize_qa_chain(force_rebuild_chain_only=False):
    global qa_system

    current_embeddings = get_embeddings()

    if not check_faiss_index_exists(FAISS_INDEX_PATH):
        logger.warning(
            f"FAISS index files not found in '{FAISS_INDEX_PATH}'. "
            "QA system will not be available until a document is uploaded and index is created."
        )
        qa_system = None
        return False

    if qa_system is not None and not force_rebuild_chain_only:
        logger.info("QA system already initialized and using existing index.")
        return True

    try:
        logger.info(f"Loading FAISS index from '{FAISS_INDEX_PATH}'...")

        loop = asyncio.get_event_loop()
        db = FAISS.load_local(
            FAISS_INDEX_PATH,
            current_embeddings,
            allow_dangerous_deserialization=True
        )

        llm = initialize_llm()

        qa_system = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=db.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=False
        )
        logger.info("QA system initialized successfully with FAISS index.")
        return True
    except Exception as e:
        logger.error(f"Failed to initialize QA system: {str(e)}", exc_info=True)
        qa_system = None
        return False


def extract_text_from_file(file_path: str) -> str:
    """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
    try:
        if file_path.lower().endswith('.pdf'):
            from pdfminer.high_level import extract_text as pdf_extract_text
            logger.info(f"Extracting text from PDF: {file_path}")
            return pdf_extract_text(file_path)
        elif file_path.lower().endswith('.md'):
            import markdown
            from bs4 import BeautifulSoup
            logger.info(f"Extracting text from Markdown: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                html = markdown.markdown(f.read())
                return BeautifulSoup(html, "html.parser").get_text()
        elif file_path.lower().endswith('.txt'):
            logger.info(f"Extracting text from TXT: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        logger.warning(f"Unsupported file format for extraction: {file_path}")
        raise ValueError("Unsupported file format for text extraction.")
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {str(e)}")
        raise


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."""
    user = update.effective_user
    await update.message.reply_html(
        rf"–ü—Ä–∏–≤–µ—Ç, {user.mention_html()}! –Ø –±–æ—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∞—à–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
    )
    await help_command(update, context)


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–æ–º–∞–Ω–¥."""
    help_text = (
        "–Ø –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n\n"
        "<b>–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:</b>\n"
        "/start - –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
        "/status - –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å QA —Å–∏—Å—Ç–µ–º—ã –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n\n"
        "<b>–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</b>\n"
        "1. –ï—Å–ª–∏ —É –º–µ–Ω—è —É–∂–µ –µ—Å—Ç—å –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–º–∞–Ω–¥–æ–π /status), –≤—ã –º–æ–∂–µ—Ç–µ —Å—Ä–∞–∑—É –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã.\n"
        "2. –ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF, TXT, MD). "
        "–Ø –æ–±—Ä–∞–±–æ—Ç–∞—é –µ–≥–æ –∏ <b>–¥–æ–ø–æ–ª–Ω—é</b> —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.\n"
        "3. –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º.\n"
    )
    await update.message.reply_html(help_text)


async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å QA —Å–∏—Å—Ç–µ–º—ã."""
    status_msg_parts = []
    if qa_system:
        status_msg_parts.append("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
        if check_faiss_index_exists(FAISS_INDEX_PATH):
            status_msg_parts.append(f"üìñ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ '{FAISS_INDEX_PATH}' –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")
        else:
            status_msg_parts.append(
                f"‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ '{FAISS_INDEX_PATH}' –ù–ï –Ω–∞–π–¥–µ–Ω–∞, —Ö–æ—Ç—è QA —Å–∏—Å—Ç–µ–º–∞ –∞–∫—Ç–∏–≤–Ω–∞ (–æ—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏?).")
    else:
        status_msg_parts.append("‚ùå QA —Å–∏—Å—Ç–µ–º–∞ –ù–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        if check_faiss_index_exists(FAISS_INDEX_PATH):
            status_msg_parts.append(
                f"‚ÑπÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ '{FAISS_INDEX_PATH}'. –ü–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ QA —Å–∏—Å—Ç–µ–º—ã...")
            if initialize_qa_chain():
                status_msg_parts.append("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
            else:
                status_msg_parts.append("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å QA —Å–∏—Å—Ç–µ–º—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏.")
        else:
            status_msg_parts.append(
                "‚ÑπÔ∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã.")

    await update.message.reply_text("\n".join(status_msg_parts))


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ FAISS –∏–Ω–¥–µ–∫—Å."""
    if not update.message.document:
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç.")
        return

    doc = update.message.document
    file_name = doc.file_name

    if not file_name.lower().endswith(('.pdf', '.md', '.txt')):
        await update.message.reply_text("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ PDF, MD –∏–ª–∏ TXT.")
        return

    processing_message = await update.message.reply_text(f"–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª: {file_name}. –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É...")

    if not os.path.exists(TEMP_UPLOAD_DIR):
        os.makedirs(TEMP_UPLOAD_DIR)

    temp_file_path = os.path.join(TEMP_UPLOAD_DIR, f"{doc.file_id}-{file_name}")

    try:
        tg_file: TelegramFile = await context.bot.get_file(doc.file_id)
        await tg_file.download_to_drive(temp_file_path)
        logger.info(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {temp_file_path}")

        await processing_message.edit_text(f"–§–∞–π–ª: {file_name}\n–ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
        content = extract_text_from_file(temp_file_path)

        if not content.strip():
            await processing_message.edit_text(f"–§–∞–π–ª: {file_name}\n–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç.")
            return

        await processing_message.edit_text(f"–§–∞–π–ª: {file_name}\n–†–∞–∑–±–∏–≤–∞—é —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=300,
            length_function=len
        )
        docs_chunks = text_splitter.split_text(content)

        if not docs_chunks:
            await processing_message.edit_text(
                f"–§–∞–π–ª: {file_name}\n–¢–µ–∫—Å—Ç –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π?).")
            return

        await processing_message.edit_text(
            f"–§–∞–π–ª: {file_name}\n–û–±–Ω–æ–≤–ª—è—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è."
        )

        current_embeddings = get_embeddings()
        db: Optional[FAISS] = None
        loop = asyncio.get_event_loop()

        if check_faiss_index_exists(FAISS_INDEX_PATH):
            try:
                logger.info(f"Loading existing FAISS index from {FAISS_INDEX_PATH} to add new documents.")
                db = await loop.run_in_executor(
                    None,
                    FAISS.load_local,
                    FAISS_INDEX_PATH,
                    current_embeddings,
                    True
                )
                logger.info("Adding new texts to the existing FAISS index...")

                await loop.run_in_executor(None, db.add_texts, docs_chunks, current_embeddings.embed_documents)
                logger.info("New documents added to existing FAISS index.")
            except Exception as e:
                logger.error(f"Failed to load or update existing FAISS index: {e}. Creating a new one.", exc_info=True)
                await processing_message.edit_text(
                    f"–§–∞–π–ª: {file_name}\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã. –°–æ–∑–¥–∞—é –Ω–æ–≤—É—é..."
                )
                db = await loop.run_in_executor(None, FAISS.from_texts, docs_chunks, current_embeddings)
        else:
            logger.info(f"FAISS index not found at {FAISS_INDEX_PATH}. Creating a new one.")
            db = await loop.run_in_executor(None, FAISS.from_texts, docs_chunks, current_embeddings)

        if db:
            if not os.path.exists(FAISS_INDEX_PATH):
                os.makedirs(FAISS_INDEX_PATH)
            logger.info(f"Saving FAISS index to {FAISS_INDEX_PATH}...")
            await loop.run_in_executor(None, db.save_local, FAISS_INDEX_PATH)
            logger.info(f"FAISS index saved to {FAISS_INDEX_PATH}")

            if initialize_qa_chain(force_rebuild_chain_only=True):
                await processing_message.edit_text(
                    f"–î–æ–∫—É–º–µ–Ω—Ç '{file_name}' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π!\n"
                    "–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –ø–æ –Ω–µ–º—É –≤–æ–ø—Ä–æ—Å—ã."
                )
            else:
                await processing_message.edit_text(
                    f"–§–∞–π–ª: {file_name}\n–î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å QA —Å–∏—Å—Ç–µ–º—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏."
                )
        else:
            await processing_message.edit_text(f"–§–∞–π–ª: {file_name}\n–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö FAISS.")

    except FileNotFoundError as e:
        logger.error(f"–û—à–∏–±–∫–∞ FileNotFoundError (–≤–µ—Ä–æ—è—Ç–Ω–æ, –º–æ–¥–µ–ª—å LLM): {str(e)}")
        await processing_message.edit_text(
            f"–û—à–∏–±–∫–∞: {str(e)}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å LLaMA —Å–∫–∞—á–∞–Ω–∞ –∏ –ø—É—Ç—å –∫ –Ω–µ–π ({MODEL_PATH}) —É–∫–∞–∑–∞–Ω –≤–µ—Ä–Ω–æ.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{file_name}': {str(e)}", exc_info=True)
        await processing_message.edit_text(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{file_name}': {str(e)}")
    finally:
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
            logger.info(f"–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_file_path}")


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ –≤–æ–ø—Ä–æ—Å –∫ QA —Å–∏—Å—Ç–µ–º–µ."""
    global qa_system
    if not update.message or not update.message.text:
        return

    question = update.message.text
    logger.info(f"–ü–æ–ª—É—á–µ–Ω –≤–æ–ø—Ä–æ—Å –æ—Ç {update.effective_user.name}: {question}")

    if qa_system is None:
        if not initialize_qa_chain():
            await update.message.reply_text(
                "–°–∏—Å—Ç–µ–º–∞ QA –Ω–µ –≥–æ—Ç–æ–≤–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç, "
                "–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ /status, –µ—Å–ª–∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞."
            )
            return

    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")

    try:
        loop = asyncio.get_event_loop()

        result = await loop.run_in_executor(None, qa_system.invoke, {"query": question})

        answer = result.get('result', "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –∏–∑ QA —Å–∏—Å—Ç–µ–º—ã.")
        source_documents = result.get('source_documents')

        logger.info(f"–û—Ç–≤–µ—Ç: {answer}")
        await update.message.reply_text(answer)

        if source_documents:
            sources_text_parts = ["\n\n<b>–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã):</b>"]
            for i, doc_source in enumerate(source_documents):
                content_preview = doc_source.page_content.replace('<', '<').replace('>', '>')
                source_info = f"{i + 1}. üìÑ ...{content_preview[:150]}..."
                sources_text_parts.append(source_info)

            sources_full_text = "\n".join(sources_text_parts)
            if len(sources_full_text) > 4096:
                sources_full_text = sources_full_text[:4090] + "..."
            await update.message.reply_html(sources_full_text)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}", exc_info=True)
        await update.message.reply_text(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}")


def main() -> None:
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞."""
    if not BOT_TOKEN or BOT_TOKEN == "TELEGRAM_BOT_TOKEN":
        logger.critical("–ù–µ —É–∫–∞–∑–∞–Ω —Ç–æ–∫–µ–Ω –±–æ—Ç–∞ (BOT_TOKEN). –ë–æ—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω.")
        return

    get_embeddings()
    if initialize_qa_chain():
        logger.info("QA —Å–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∏–Ω–¥–µ–∫—Å–æ–º.")
    else:
        logger.warning("QA —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –±—ã–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ. "
                       "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è.")

    application = Application.builder().token(BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("status", status_command))
    application.add_handler(MessageHandler(filters.Document.ALL, handle_document))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    logger.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    application.run_polling()


if __name__ == "__main__":
    main()